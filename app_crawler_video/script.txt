App Crawler is a tool to scan the native application to find all broken links, broken images, and to create a sitemap of the screens that were crawled. On the right side is an android device emulator, while the left side has the output result file and a log file to store the crawling actions. The script is invoked from the command line by providing the username and environment as arguments. When executed, the crawler first attempts to log-in to the application.((⏱️=5000))After logging in, it extracts all the CTAs in the screens and adds them to a stack. It then checks if there are any broken images in the screen. The crawler is also capable of scrolling; which ensures that all possible images are checked. To verify that every CTA on a screen is covered, the crawler follows a depth-first-search approach for clicking the CTAs. At any point, the last CTA from the stack is popped and clicked. In this case, it was the Menu button on the bottom right corner. After clicking on any CTA, the crawler waits some time for the page to load. Once the page has been loaded, the crawler again follows the process of extracting all the CTAs, adding them to the stack and finding out broken images in the screen. The page source for any screen is the result of an API call. Some of these calls take more time than others to provide a response, and render it on the screen. These kinds of cases are also covered by having a polling logic to check if a screen has loaded or not every few seconds. 
As visible in the log file below, the crawler is checking if the page has loaded or not. There are a maxmimum of 18 retries, with checks being performed every 5 seconds. Overall, if there is no change in the page for 90 seconds, it will be reported as a blank page. While we wait for the page to load, let me go over the depth-first-search logic that I mentioned previously. For the DFS logic to work, there must also be cases where the crawler traverses back to a previous screen. In case of any error screens or blank screens, the crawler stores the screen as an error and traverses back to the previous screen. Similarly, if any CTA leads us outside the application, for example, the browser, the crawler extracts the URL, sends and HTTP request and stores the response code before returning back to the previous screen. And of course, if all CTAs in a screen have been checked, the crawler traverses back to the previous screen. This entire traversal is being stored in the result file. This file along with the error report is then used to generate the sitemap for the. For the crawler, screen that are rendered as webview belong to the same class as the native application screens. This means that the webview screens are also scanned for any broken images or links.
As visible, the crawler is checking if the page has loaded or not. There are a few retries left until it is considered as an unloaded screen. As there was a timeout, this screen was stored as a blank screen and we will be traversing back to the previous screen, which is the menu screen.
((⏱️=10000))Once the crawler is back in the Menu screen and has validated the the screen has loaded, it checks the next CTA to consider. As the Menu and Explore CTAs have been covered, the next one is the Pay and Transfer button. Again, the crawler waits for some time for the page to load. Once the page has loaded, it extracts all the CTAs, checks if there are any broken links. As the Menu and Explore buttons are duplicate and have already been covered, these will not be actioned on again. The crawler is smart enough to know that the next unchecked CTA is the Depsit button. I hope you were able to get a general idea as to how the app crawler is working recursively. The crawling process will stop automatically when the stack has been emptied completely. ((⏱️=10000)) During any point in the crawling process, it is possible to generate the sitemap and error report for the screens. Like the crawler, these can also be run from the command line itself.
((⏱️=10000))
The sitemap is an interactive visualisation. Each screen is represented by nodes and each action is represented by arrows. You can hover on nodes to display the screen image, and click on the nodes to show the screens that are accessible from it. The arrow contain the names of the CTAs that were clicked during the crawling process. Clicking on expanded nodes will collapse the children, thus giving a clear view on the traversal between the screens.((⏱️=10000))
Let me show you the error report now. The report shows if there are any broken images in any screens. It also shows which CTAs/ links led to error screens, and what exactly the error was.

